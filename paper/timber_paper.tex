\documentclass[11pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{authblk}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
}

\title{\textbf{Timber: An Ahead-of-Time Compiler and Serving Infrastructure for Classical Machine Learning Inference}}

\author[1]{Kossiso Royce}
\affil[1]{Electricsheep Africa}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present \textbf{Timber}, an ahead-of-time (AOT) compiler and serving infrastructure for classical machine learning models. Timber ingests trained model artifacts from five major frameworks---XGBoost, LightGBM, scikit-learn, CatBoost, and ONNX---compiles them through a framework-agnostic intermediate representation (IR) and six domain-specific optimization passes, and emits native inference code targeting C99, WebAssembly, or MISRA-C-compliant output. Timber also provides an Ollama-style workflow: a single \texttt{timber load} command compiles and caches a model locally, and \texttt{timber serve} exposes it over a REST API with sub-100\,$\mu$s warm inference latency. On a breast cancer classification benchmark (50 trees, depth 4, 30 features), Timber achieves \textbf{336$\times$ lower P50 latency} (2\,$\mu$s vs.\ 672\,$\mu$s) and \textbf{533$\times$ higher throughput} compared to XGBoost's Python inference, while maintaining bit-exact numerical agreement ($<10^{-5}$ maximum absolute error). The generated code requires no runtime dependencies, no dynamic allocation, and no recursion. Timber bridges the gap between training in Python and deploying at native speed---one command to load, one command to serve.
\end{abstract}

%% ======================================================================
\section{Introduction}
%% ======================================================================

Classical machine learning models---gradient-boosted trees (GBTs), random forests, and decision tree pipelines---remain the dominant workhorses of production ML in finance, healthcare, fraud detection, ad tech, and industrial applications~\cite{xgboost,lightgbm,catboost}. Despite their simplicity relative to deep neural networks, deploying these models in latency-sensitive environments carries significant overhead: Python interpreter startup, framework runtime initialization, memory allocation for intermediate structures, GIL contention under concurrent load, and dependency management complexity.

The deep-learning ecosystem solved its serving problem with optimized runtimes (TensorRT, TVM, Triton). More recently, the LLM ecosystem adopted a radically simple user experience through tools like Ollama~\cite{ollama}: \texttt{ollama pull}, \texttt{ollama run}. Classical ML has no equivalent. Models trained in XGBoost or scikit-learn are typically deployed behind a Flask endpoint with the entire Python runtime---a 200\,MB process serving a 2\,MB model.

Timber addresses this gap from two angles:

\begin{enumerate}
    \item \textbf{Compilation.} Timber is an AOT compiler that treats a trained model as a program specification and applies compiler techniques to produce minimal, deterministic, zero-dependency native inference code.
    \item \textbf{Serving.} Timber provides an Ollama-style CLI and HTTP server: one command to compile and cache a model, one command to serve it over a REST API with sub-100\,$\mu$s inference latency.
\end{enumerate}

\noindent The key contributions are:

\begin{enumerate}
    \item A typed, framework-agnostic IR for inference pipelines with support for multi-stage models, ensemble metadata, and vectorization annotations (\S\ref{sec:ir}).
    \item Front-end parsers for five frameworks: XGBoost, LightGBM, scikit-learn, CatBoost, and ONNX (\S\ref{sec:frontends}).
    \item Six domain-specific optimization passes exploiting tree ensemble structure (\S\ref{sec:optimizer}).
    \item Three code generation backends: C99, WebAssembly (WAT), and MISRA-C for safety-critical deployment (\S\ref{sec:codegen}).
    \item A local model store and HTTP inference server providing an Ollama-style developer experience (\S\ref{sec:serving}).
    \item Differential compilation enabling incremental rebuilds when models are updated (\S\ref{sec:diff}).
    \item Empirical validation showing 300--500$\times$ speedup with bit-exact numerical fidelity (\S\ref{sec:eval}).
\end{enumerate}

%% ======================================================================
\section{System Design}
%% ======================================================================

\subsection{Architecture Overview}

Timber follows a classical compiler architecture extended with a model store and serving layer:

\begin{equation*}
\text{Artifact} \xrightarrow{\text{parse}} \text{IR} \xrightarrow{\text{optimize}} \text{IR}' \xrightarrow{\text{emit}} \text{C99/WASM} \xrightarrow{\text{compile}} \text{.so} \xrightarrow{\text{serve}} \text{HTTP}
\end{equation*}

\noindent The compilation pipeline consists of three phases:

\begin{enumerate}
    \item \textbf{Front-end}: Format-specific parsers convert model artifacts into a canonical IR. Auto-detection selects the appropriate parser based on file extension and content inspection.
    \item \textbf{Middle-end}: A configurable sequence of six optimization passes transforms the IR, each producing an auditable change log.
    \item \textbf{Back-end}: Code emitters produce self-contained source code for the target platform (C99, WebAssembly, or MISRA-C).
\end{enumerate}

\noindent The serving layer adds:

\begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{Model store}: A local registry (\texttt{\~{}/.timber/models/}) that caches compiled artifacts by name.
    \item \textbf{HTTP server}: An Ollama-style REST API exposing compiled models for inference.
\end{enumerate}

\subsection{Intermediate Representation}
\label{sec:ir}

The Timber IR represents an inference pipeline as an ordered sequence of \emph{stages}, each a typed dataclass:

\begin{itemize}
    \item \textbf{ScalerStage}: Elementwise affine transform $(x - \mu_i) / \sigma_i$ with per-feature means and scales.
    \item \textbf{TreeEnsembleStage}: A collection of decision trees with split conditions, leaf values, objective type, learning rate, base score, and an \texttt{annotations} dictionary for optimization metadata.
    \item \textbf{EncoderStage}: Categorical-to-numeric transforms (ordinal, one-hot, target encoding).
    \item \textbf{VotingEnsembleStage}: Weighted aggregation across multiple sub-model IRs.
    \item \textbf{StackingEnsembleStage}: Meta-learner over sub-model predictions with optional input passthrough.
\end{itemize}

Each \texttt{Tree} is stored as a flat array of \texttt{TreeNode} structures with explicit parent--child indices, feature index, threshold, default-left flag, and leaf value. This representation enables $O(1)$ random access during optimization passes and efficient serialization. The IR supports full JSON round-tripping for reproducibility and debugging.

The \texttt{Objective} enum captures the model's task type: \texttt{REGRESSION}, \texttt{BINARY\_CLASSIFICATION}, or \texttt{MULTICLASS\_CLASSIFICATION}. This drives the back-end's choice of activation function (identity, sigmoid, or softmax) and output dimensionality.

%% ======================================================================
\section{Front-End Parsers}
\label{sec:frontends}
%% ======================================================================

Timber supports five model formats through dedicated parsers. All parsers produce the same IR, enabling cross-framework optimization and code generation.

\subsection{XGBoost JSON}

The XGBoost parser handles the JSON model dump format (version $\geq$ 2.0), extracting parallel arrays of split indices, thresholds, child pointers, and default-left flags. A critical detail: XGBoost stores \texttt{base\_score} in probability space for logistic objectives; Timber applies the logit transform $\text{logit}(p) = \ln(p/(1-p))$ to convert to margin space for correct accumulation.

\subsection{LightGBM Text}

The LightGBM parser processes the \texttt{model.txt} format, parsing tree structures from the text-based array representation with negative-indexed leaf references. Input validation rejects empty files and malformed content with descriptive error messages.

\subsection{scikit-learn Pickle}

The scikit-learn parser deserializes \texttt{.pkl} files and extracts tree structures from fitted estimators. Supported model types include:

\begin{itemize}
    \item \texttt{GradientBoostingClassifier/Regressor}
    \item \texttt{RandomForestClassifier/Regressor}
    \item \texttt{HistGradientBoostingClassifier/Regressor}
    \item \texttt{DecisionTreeClassifier/Regressor}
    \item \texttt{Pipeline} with \texttt{StandardScaler} + tree estimator
\end{itemize}

\noindent For pipelines containing a \texttt{StandardScaler}, the parser emits a \texttt{ScalerStage} followed by a \texttt{TreeEnsembleStage}, preserving the pipeline structure for potential fusion by the optimizer.

\subsection{CatBoost JSON}

CatBoost uses \emph{oblivious decision trees}---symmetric trees where every node at the same depth uses the same split feature and threshold. The parser converts CatBoost's JSON format into the Timber IR's general tree representation, expanding the oblivious structure into explicit internal nodes and leaves. Splits are stored bottom-up in CatBoost's format; the parser reverses this ordering during tree construction.

\subsection{ONNX ML Opset}

The ONNX parser handles \texttt{TreeEnsembleClassifier} and \texttt{TreeEnsembleRegressor} operators from the ONNX ML opset. It extracts the flat node arrays (node IDs, feature IDs, thresholds, modes, tree IDs) and reconstructs the tree structures. This enables Timber to accept models exported from any ONNX-compatible framework.

\subsection{Auto-Detection}

The \texttt{detect\_format} function identifies the model format from file extension and content:

\begin{itemize}
    \item \texttt{.json}: Inspects keys (\texttt{learner} $\to$ XGBoost, \texttt{oblivious\_trees} $\to$ CatBoost)
    \item \texttt{.txt}, \texttt{.model}: LightGBM text format
    \item \texttt{.pkl}, \texttt{.pickle}: scikit-learn pickle
    \item \texttt{.onnx}: ONNX protobuf
\end{itemize}

%% ======================================================================
\section{Optimization Passes}
\label{sec:optimizer}
%% ======================================================================

Timber implements six optimization passes, each operating on the IR and producing a documented change log for audit purposes. Passes are executed sequentially by the \texttt{OptimizerPipeline}, and each reports whether it made changes and the resulting IR.

\subsection{Pass 1: Dead Leaf Elimination}

Leaves whose absolute contribution is below a configurable threshold $\epsilon$ relative to the maximum leaf value are pruned:

\begin{equation}
\text{prune if } |v_\ell| < \epsilon \cdot \max_{\ell'} |v_{\ell'}|
\end{equation}

When both children of an internal node are pruned, the node collapses to a leaf with the average value. This reduces code size and inference-time branch traversals.

\subsection{Pass 2: Constant Feature Detection}

When both children of an internal node are leaves with identical values, the split is redundant and the node is folded into a single leaf. This occurs when a feature provides no discriminative power at a particular node---common in models trained with early stopping.

\subsection{Pass 3: Threshold Quantization}

For each feature, all split thresholds across all trees are analyzed to determine the minimum precision required:

\begin{itemize}
    \item \textbf{int8}: All thresholds are integers in $[-128, 127]$.
    \item \textbf{int16}: All thresholds are integers in $[-32768, 32767]$.
    \item \textbf{float16}: Fewer than 4 significant decimal digits.
    \item \textbf{float32}: Default.
\end{itemize}

\noindent This metadata is stored in the IR annotations and is available to SIMD-specialized backends.

\subsection{Pass 4: Frequency-Ordered Branch Sorting}

Given calibration data, each internal node's left/right branch frequency is counted. If the less-taken branch is currently the fall-through path, the children are swapped (with threshold adjustment) to improve branch prediction and instruction cache utilization on pipelined architectures.

\subsection{Pass 5: Pipeline Fusion}

When a \texttt{ScalerStage} (with per-feature mean $\mu_i$ and scale $\sigma_i$) precedes a \texttt{TreeEnsembleStage}, the scaler is absorbed into the tree thresholds:

\begin{equation}
\theta'_{i} = \theta_{i} \cdot \sigma_i + \mu_i
\end{equation}

\noindent This eliminates the runtime scaling step entirely, reducing the pipeline to a single stage with zero overhead for preprocessing.

\subsection{Pass 6: Vectorization Analysis}

The vectorization pass analyzes the tree ensemble to identify opportunities for SIMD-batched inference. For each tree, it computes:

\begin{itemize}
    \item \textbf{Depth profile}: Maximum and average depth across all trees.
    \item \textbf{Feature access order}: The sequence of feature indices accessed along root-to-leaf paths.
    \item \textbf{Feature frequency}: How often each feature appears in split nodes across the ensemble.
    \item \textbf{Structure groups}: Sets of trees with identical topology (same depth, same branching pattern), which can be evaluated in lockstep with SIMD instructions.
\end{itemize}

\noindent The pass produces a \texttt{VectorizationHint} stored in the ensemble's \texttt{annotations} field, including a recommended batch tile size. Trees with uniform shallow depth ($\leq 3$) receive larger tile recommendations, as their traversal maps well to SIMD gather/scatter patterns.

%% ======================================================================
\section{Code Generation}
\label{sec:codegen}
%% ======================================================================

Timber supports three code generation backends, each targeting a different deployment scenario.

\subsection{C99 Emitter (Primary)}

The primary back-end produces five files: \texttt{model.h} (public API with ABI version), \texttt{model\_data.c} (static const arrays), \texttt{model.c} (inference logic), \texttt{CMakeLists.txt}, and \texttt{Makefile}.

\subsubsection{Design Guarantees}

\begin{itemize}
    \item \textbf{No dynamic allocation}: All data is \texttt{static const}; the context struct is statically allocated.
    \item \textbf{No recursion}: Tree traversal uses iterative \texttt{while} loops with bounded iteration count equal to tree depth.
    \item \textbf{Double-precision accumulation}: Tree output sums use \texttt{double} before casting to \texttt{float} at output, matching framework-native precision.
    \item \textbf{NaN handling}: Missing values follow the \texttt{default\_left} path, matching XGBoost/LightGBM semantics.
    \item \textbf{Thread safety}: The context is read-only after initialization; concurrent \texttt{timber\_infer} calls are safe.
    \item \textbf{ABI versioning}: \texttt{TIMBER\_ABI\_VERSION} enables runtime compatibility checks.
    \item \textbf{Error codes}: All functions return typed error codes (\texttt{TIMBER\_OK}, \texttt{TIMBER\_ERR\_NULL}, etc.) with \texttt{timber\_strerror()} for human-readable messages.
\end{itemize}

\subsubsection{Runtime Logging}

The generated code includes an optional logging callback:

\begin{lstlisting}[language=C,caption={Runtime logging API}]
typedef void (*timber_log_fn)(
    int level, const char* msg);
void timber_set_log_callback(
    timber_log_fn fn);
const char* timber_strerror(int code);
\end{lstlisting}

\noindent Levels follow syslog convention: 0 = error, 1 = warn, 2 = info, 3 = debug. When no callback is set, logging is a no-op with zero overhead.

\subsubsection{Multi-class Classification}

For multi-class objectives, the emitter generates an inline softmax with double-precision accumulation:

\begin{equation}
p_k = \frac{\exp(s_k - \max_j s_j)}{\sum_{j} \exp(s_j - \max_j s_j)}
\end{equation}

\noindent where $s_k$ is the accumulated score for class $k$. The $\max$-subtraction ensures numerical stability.

\subsubsection{Generated API}

\begin{lstlisting}[language=C,caption={Public C API}]
int timber_init(TimberCtx** ctx);
void timber_free(TimberCtx* ctx);
int timber_infer(const float* inputs,
    int n_samples, float* outputs,
    const TimberCtx* ctx);
int timber_infer_single(
    const float inputs[N_FEATURES],
    float outputs[N_OUTPUTS],
    const TimberCtx* ctx);
\end{lstlisting}

\subsection{WebAssembly Emitter}

The WASM back-end produces WebAssembly Text Format (WAT) and JavaScript bindings for browser and edge deployment. The emitter generates:

\begin{itemize}
    \item A \texttt{model.wat} file with linear memory layout for tree data, a \texttt{\$traverse\_tree} function, and the main \texttt{\$timber\_infer\_single} export.
    \item A \texttt{timber\_model.js} file providing \texttt{loadTimberModel()} which instantiates the WASM module, writes inputs to linear memory, invokes inference, and reads outputs---exposing a simple \texttt{predict(features)} API.
\end{itemize}

\noindent For binary classification, the emitter includes a sigmoid activation ($\sigma(x) = 1/(1+e^{-x})$) implemented via an \texttt{\$exp\_neg} helper in WAT.

\subsection{MISRA-C Compliance Mode}

For safety-critical deployment (automotive, medical devices, avionics), Timber provides a MISRA-C:2012 compliant emitter. The \texttt{MisraCEmitter} wraps the standard C99 emitter and applies post-processing transformations:

\begin{itemize}
    \item Unsigned integer suffix enforcement (Rule 10.1)
    \item No compiler extensions (Rule 1.1)
    \item No redefinition of standard identifiers (Rule 21.1)
    \item Compliance marker in generated headers
\end{itemize}

\noindent A built-in compliance checker validates the generated code against seven MISRA rules and produces a \texttt{MisraReport} with violation and warning counts.

%% ======================================================================
\section{Serving Infrastructure}
\label{sec:serving}
%% ======================================================================

Inspired by Ollama's developer experience for LLMs, Timber provides an end-to-end workflow for classical ML models: load once, serve anywhere.

\subsection{Model Store}

The model store (\texttt{\~{}/.timber/models/}) is a local registry that caches compiled artifacts. When the user runs \texttt{timber load model.json ---name fraud-detector}, Timber:

\begin{enumerate}
    \item Auto-detects the model format.
    \item Parses the artifact into the IR.
    \item Runs the full optimization pipeline (6 passes).
    \item Emits C99 source code.
    \item Compiles a shared library (\texttt{.so}/\texttt{.dylib}) via \texttt{gcc}.
    \item Stores everything under \texttt{\~{}/.timber/models/fraud-detector/} with a \texttt{model\_info.json} registry entry.
\end{enumerate}

\noindent Subsequent \texttt{timber serve fraud-detector} calls load the pre-compiled library instantly---no recompilation.

\subsection{CLI Interface}

\begin{lstlisting}[language=bash,caption={Ollama-style CLI}]
# Load and compile a model
timber load model.json --name my-model

# List cached models
timber list

# Serve over HTTP (port 11434)
timber serve my-model

# Remove a cached model
timber remove my-model
\end{lstlisting}

\subsection{HTTP API}

The server exposes an Ollama-compatible REST API:

\begin{table}[h]
\centering
\caption{HTTP API endpoints}
\label{tab:api}
\begin{tabular}{lll}
\toprule
\textbf{Endpoint} & \textbf{Method} & \textbf{Description} \\
\midrule
\texttt{/api/predict}   & POST & Run inference \\
\texttt{/api/generate}  & POST & Alias (Ollama compat) \\
\texttt{/api/models}    & GET  & List loaded models \\
\texttt{/api/model/:id} & GET  & Model metadata \\
\texttt{/api/health}    & GET  & Health check \\
\bottomrule
\end{tabular}
\end{table}

\noindent The predict endpoint accepts JSON:

\begin{lstlisting}[language=bash,caption={Inference request}]
curl http://localhost:11434/api/predict \
  -d '{"model": "fraud-detector",
       "inputs": [[1.0, 2.0, ...]]}'
\end{lstlisting}

\noindent and returns:

\begin{lstlisting}[language=bash,caption={Inference response}]
{"model": "fraud-detector",
 "outputs": [0.97],
 "n_samples": 1,
 "latency_us": 91.0,
 "done": true}
\end{lstlisting}

\noindent The server handles the HTTP envelope in Python; the actual inference call goes directly to the compiled C shared library via \texttt{ctypes}. This architecture means Python is never in the inference hot path---it only parses JSON and copies buffers.

\subsection{Serving Latency}

On a MacBook Pro (Apple M-series), the end-to-end HTTP round-trip for a 50-tree breast cancer model measured 91\,$\mu$s warm latency at the \texttt{/api/predict} endpoint (measured via \texttt{curl}). The C inference itself is $\sim$2\,$\mu$s; the remainder is HTTP parsing and JSON serialization.

%% ======================================================================
\section{Differential Compilation}
\label{sec:diff}
%% ======================================================================

In production, models are retrained frequently. Recompiling the entire model on every update is wasteful when only a few trees have changed. Timber's differential compilation module addresses this:

\begin{enumerate}
    \item \textbf{Tree hashing}: Each tree is assigned a content hash based on its structure (node IDs, feature indices, thresholds, leaf values, child pointers).
    \item \textbf{Diff computation}: Given two IR instances (old and new), Timber identifies added, removed, modified, and unchanged trees by comparing hash sets.
    \item \textbf{Incremental assembly}: The new IR is annotated with diff metadata, enabling downstream tooling to perform selective recompilation.
\end{enumerate}

\noindent This is particularly valuable in model monitoring pipelines where a model is retrained hourly on fresh data---typically only 5--20\% of trees change between versions.

%% ======================================================================
\section{Ensemble Composition}
%% ======================================================================

Timber supports composing multiple compiled models into higher-order ensembles:

\begin{itemize}
    \item \textbf{Voting ensembles}: Weighted soft or hard vote across $N$ sub-models. Each sub-model is a complete Timber IR; the \texttt{VotingEnsembleStage} aggregates their predictions with configurable weights.
    \item \textbf{Stacking ensembles}: A meta-learner (itself a Timber IR) trained on the outputs of $N$ base models. The \texttt{StackingEnsembleStage} supports optional passthrough of original features.
\end{itemize}

\noindent These compositions are first-class IR stages, enabling the full optimization pipeline to operate on the ensemble structure.

%% ======================================================================
\section{Evaluation}
\label{sec:eval}
%% ======================================================================

\subsection{Experimental Setup}

We evaluate Timber on the UCI Breast Cancer Wisconsin dataset (569 samples, 30 features) using an XGBoost binary classifier (50 trees, max depth 4, learning rate 0.1). The compiled model targets x86\_64 with \texttt{gcc -O3 -std=c99}. The Python baseline uses XGBoost 2.0+ with \texttt{booster.predict()}. All benchmarks use 1,000 warmup iterations followed by 10,000 timed iterations, reporting P50/P95/P99 latency and throughput.

\subsection{Single-Sample Latency}

\begin{table}[h]
\centering
\caption{Single-sample inference latency ($\mu$s)}
\label{tab:latency}
\begin{tabular}{lrrrr}
\toprule
\textbf{Method} & \textbf{P50} & \textbf{P95} & \textbf{P99} & \textbf{Tput} \\
\midrule
XGBoost Python & 672 & 2,776 & 13,468 & 840/s \\
Timber C99     & 2   & 2     & 3      & 447,868/s \\
\midrule
\textbf{Speedup} & \textbf{336$\times$} & \textbf{1,388$\times$} & \textbf{4,489$\times$} & \textbf{533$\times$} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Batch Inference}

\begin{table}[h]
\centering
\caption{Batch inference latency ($\mu$s, batch size = 10)}
\label{tab:batch}
\begin{tabular}{lrrrr}
\toprule
\textbf{Method} & \textbf{P50} & \textbf{P95} & \textbf{P99} & \textbf{Tput} \\
\midrule
XGBoost Python & 521 & 1,305 & 13,151 & 11,311/s \\
Timber C99     & 15  & 39    & 163    & 475,195/s \\
\midrule
\textbf{Speedup} & \textbf{35$\times$} & \textbf{33$\times$} & \textbf{81$\times$} & \textbf{42$\times$} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Numerical Accuracy}

\begin{table}[h]
\centering
\caption{Numerical accuracy vs.\ XGBoost Python (114 test samples)}
\label{tab:accuracy}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Max absolute error & $< 10^{-5}$ \\
Mean absolute error & $< 10^{-6}$ \\
Divergent samples ($> 10^{-5}$) & 0 / 114 \\
Classification agreement & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\noindent The key to achieving bit-exact results is twofold: (1)~converting XGBoost's probability-space \texttt{base\_score} to logit space via $\text{logit}(p) = \ln(p/(1-p))$, and (2)~using \texttt{double} accumulation for the tree output sum before the final sigmoid/softmax and cast to \texttt{float}.

\subsection{Tail Latency}

Timber's P99 latency (3\,$\mu$s) is 4,489$\times$ lower than XGBoost Python's P99 (13,468\,$\mu$s). The Python runtime's garbage collection, GIL, and DMatrix construction create extreme tail latency; Timber's deterministic code path eliminates these sources entirely.

\subsection{Artifact Size}

For the 50-tree breast cancer model:

\begin{table}[h]
\centering
\caption{Deployment artifact comparison}
\label{tab:size}
\begin{tabular}{lr}
\toprule
\textbf{Artifact} & \textbf{Size} \\
\midrule
Timber compiled .so & 48 KB \\
Timber C99 source & 58 KB \\
XGBoost Python runtime & $\sim$50 MB \\
\bottomrule
\end{tabular}
\end{table}

\noindent The compiled artifact is approximately \textbf{1,000$\times$ smaller} than the Python runtime required to serve the same model.

\subsection{Framework Coverage}

We validate correct parsing and compilation across all five supported frameworks:

\begin{table}[h]
\centering
\caption{Test coverage by component (144 tests total)}
\label{tab:tests}
\begin{tabular}{lr}
\toprule
\textbf{Component} & \textbf{Tests} \\
\midrule
XGBoost parser & 7 \\
LightGBM parser & (via rigorous) \\
scikit-learn parser & 7 \\
CatBoost parser & 7 \\
WASM emitter & 5 \\
Vectorization pass & 6 \\
Diff compilation & 6 \\
MISRA-C compliance & 4 \\
Stacking/voting ensembles & 4 \\
Model store + CLI & 18 \\
Auto-detect + formats & 4 \\
Multi-class softmax & 5 \\
Fuzz testing (robustness) & 17 \\
Rigorous end-to-end & 13 \\
\midrule
\textbf{Total} & \textbf{144} \\
\bottomrule
\end{tabular}
\end{table}

%% ======================================================================
\section{Production Considerations}
%% ======================================================================

\subsection{Drop-in Predictor}

Timber provides a Python \texttt{ctypes} wrapper (\texttt{TimberPredictor}) that serves as a drop-in replacement for framework \texttt{predict()} methods:

\begin{lstlisting}[language=Python,caption={Drop-in prediction}]
from timber.runtime import TimberPredictor

pred = TimberPredictor.from_model("model.json")
y = pred.predict(X)  # numpy in/out
\end{lstlisting}

\subsection{Audit Trail}

Every compilation produces a deterministic JSON audit report containing: input artifact SHA-256 hash, compiler version, optimization pass log with per-pass timing, output file hashes, model summary, and target specification. This supports regulatory review in financial and healthcare applications.

\subsection{Limitations}

\begin{itemize}
    \item The WASM emitter produces WAT text; binary encoding (\texttt{.wasm}) requires an external tool (e.g., \texttt{wat2wasm}).
    \item Inter-tree optimizations (tree merging, ensemble distillation) are not yet implemented.
    \item The CatBoost parser supports oblivious trees only; non-oblivious CatBoost models require conversion.
    \item The HTTP server uses Python's \texttt{http.server}; production deployments at scale should front this with a reverse proxy.
    \item SIMD code generation from vectorization hints is analyzed but not yet emitted; the current code relies on the C compiler's auto-vectorization.
\end{itemize}

%% ======================================================================
\section{Related Work}
%% ======================================================================

\textbf{Treelite}~\cite{treelite} compiles tree ensembles to C but uses monolithic code generation without an intermediate representation or optimization passes. It supports XGBoost and LightGBM but not scikit-learn pipelines or CatBoost.

\textbf{Hummingbird}~\cite{hummingbird} converts tree models to tensor computations, running inference through PyTorch or ONNX Runtime. This enables GPU acceleration but introduces significant runtime dependencies.

\textbf{lleaves}~\cite{lleaves} uses LLVM to JIT-compile LightGBM models. While achieving native performance, it is framework-specific and requires the LLVM toolchain at deployment time.

\textbf{ONNX Runtime}~\cite{onnxruntime} provides generic ML inference with the TreeEnsemble ML operator, but incurs dynamic dispatch overhead and requires the full runtime.

\textbf{Ollama}~\cite{ollama} pioneered the ``load and serve'' developer experience for LLMs. Timber adapts this UX paradigm for classical ML: \texttt{timber load} replaces \texttt{ollama pull}, and \texttt{timber serve} replaces \texttt{ollama run}.

Timber differentiates by providing: (a)~a framework-agnostic IR enabling cross-framework optimization, (b)~six domain-specific optimizer passes, (c)~three code generation backends (C99, WASM, MISRA-C), (d)~an Ollama-style serving workflow, (e)~differential compilation for incremental updates, and (f)~deterministic audit trails for regulated environments.

%% ======================================================================
\section{Conclusion}
%% ======================================================================

Timber demonstrates that applying classical compiler techniques to trained ML models yields dramatic inference performance improvements---300--500$\times$ over Python-based inference---with bit-exact numerical fidelity. By combining AOT compilation with an Ollama-style developer experience, Timber makes native-speed inference as simple as:

\begin{lstlisting}[language=bash]
pip install timber
timber load model.json --name my-model
timber serve my-model
\end{lstlisting}

\noindent The generated code is zero-allocation, zero-dependency C99 suitable for the most constrained deployment targets: embedded systems, edge devices, latency-critical microservices, and regulated environments requiring full auditability. With support for five input frameworks, three output targets, and a comprehensive optimization pipeline, Timber bridges the gap between training in Python and deploying at native speed.

\begin{thebibliography}{9}

\bibitem{treelite}
H.~Cho et al., ``Treelite: Toolbox for decision tree deployment,'' 2018. \url{https://github.com/dmlc/treelite}

\bibitem{hummingbird}
S.~Nakandala et al., ``A Tensor Compiler for Unified Machine Learning Prediction Serving,'' OSDI, 2020.

\bibitem{lleaves}
S.~Boehm, ``lleaves: Compiled LightGBM Models,'' 2021. \url{https://github.com/siboehm/lleaves}

\bibitem{onnxruntime}
Microsoft, ``ONNX Runtime: Cross-platform, high performance ML inferencing and training accelerator,'' 2019. \url{https://onnxruntime.ai}

\bibitem{xgboost}
T.~Chen and C.~Guestrin, ``XGBoost: A Scalable Tree Boosting System,'' KDD, 2016.

\bibitem{lightgbm}
G.~Ke et al., ``LightGBM: A Highly Efficient Gradient Boosting Decision Tree,'' NeurIPS, 2017.

\bibitem{catboost}
L.~Prokhorenkova et al., ``CatBoost: Unbiased boosting with categorical features,'' NeurIPS, 2018.

\bibitem{ollama}
Ollama, ``Get up and running with large language models,'' 2023. \url{https://ollama.ai}

\bibitem{sklearn}
F.~Pedregosa et al., ``Scikit-learn: Machine Learning in Python,'' JMLR, vol.~12, pp.~2825--2830, 2011.

\end{thebibliography}

\end{document}
